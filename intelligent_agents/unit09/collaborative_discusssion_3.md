# Collaborative Discussion 3: Deep Learning
{: .hidden-title }

## Brief

The advent of new technologies supported by Deep Learning models mean that it is now possible to generate ‘new’ content, for example, Dall-E AI to generate images or ChatGPT to create prose.

Do you think that these new technologies offer any ethical issues that should be considered, and if not, why not?

#### Learning Outcomes
- The knowledge and skills required to develop, deploy and evaluate the tools and techniques of intelligent systems to solve real-world problems.


## Navigation
- [Initial Post](#initial-post)
  - [Peer Response 1](#peer-response-1)
  - [Peer Response 2](#peer-response-2)
- [Summary Post](#summary-post)
- [Back to Intelligent Agents](/intelligent_agents/)


## Initial Post


The advent of deep learning-based tools like DALL-E and ChatGPT means we can generate new images, text, and art that previously needed human creativity. That is exciting; however, there are ethical issues that deserve careful thought. One concern is bias: what models generate often reflects biased training data, stereotypes, or under‑representation of certain groups. For example, visual tools tend to portray certain professions with gender bias, reinforcing harmful norms (Sandoval-Martin and Martínez-Sanzo, 2024).

Another is authenticity and originality. Is it clear whether the authorship is human, artificial intelligence (AI), or both? If machines do more content creation, does that reduce opportunities for human artists, writers, and designers? That matters for credit, fair compensation, law, and for how people trust content (Vlaad, 2024). With generative models being capable of producing highly realistic but false content, there is a risk that these images or text may be used to mislead, defame, or manipulate (Bendel, 2025). Irrespective of intent, in many cases, training data is sourced from multiple places, and privacy rights are not always respected, meaning individuals may not be aware that their works or likenesses were used without their permission.

It may be possible to manage these risks with transparency, regulation, human oversight and ethical guidelines (Szadeczky and Bederna, 2025). If use is limited to well‐defined contexts, with acknowledgement of AI involvement, then many harms can be reduced. However, overlooking ethical issues entirely could lead to serious consequences, especially given the rapid spread of these tools.

 
#### References

Bendel, O. (2025) ‘Image synthesis from an ethical perspective’, AI & SOCIETY, 40(2), pp. 437–446. Available at: https://doi.org/10.1007/s00146-023-01780-4.

Sandoval-Martin, T. and Martínez-Sanzo, E. (2024) ‘Perpetuation of Gender Bias in Visual Representation of Professions in the Generative AI Tools DALL·E and Bing Image Creator’, Social Sciences, 13(5), p. 250. Available at: https://doi.org/10.3390/socsci13050250.

Szadeczky, T. and Bederna, Z. (2025) ‘Risk, regulation, and governance: evaluating artificial intelligence across diverse application scenarios’, Security Journal, 38(1), p. 35. Available at: https://doi.org/10.1057/s41284-025-00495-z.

Vlaad, S. (2024) ‘A portrait of the artist as a young algorithm’, Ethics and Information Technology, 26(3), p. 58. Available at: https://doi.org/10.1007/s10676-024-09796-0.



[Back to the top](#)

## Peer Responses

### Peer response 1
Jaco Espag spotlights several serious ethical concerns surrounding generative AI, including how it influences bias, questions of authorship, and its potential for misuse. Bias is perhaps the most pressing issue, since stereotypes encoded in AI-generated work could exacerbate social divisions. Research indicates that models like DALL·E frequently replicate damaging stereotypes in their outputs without corrective mechanisms (Stock et al., 2022; Sandoval-Martin and Martínez-Sanzo, 2024). Recruitment has faced a similar issue of copying or even entrenching prejudices within society in AI systems (Rigotti and Fosch-Villaronga, 2024).

The matter of authorship and originality is also key. AI's increasing presence in cultural production, as Espag points out, presents challenging questions about things such as credit, fair payment for labour and authenticity. Vlaad (2024) raises comparable considerations and suggests that the more human creativity is progressively entangled with mechanical processes, the harder it is to assess the value of artistic production or human society (where art is concerned). Faith in digital media is also being eroded by generative AI's capacity for creating photorealistic but false or defamatory material (Bendel, 2025).

Privacy risks compound these challenges, since training datasets are often scraped in bulk from online sources without consent. This raises important questions about the fair use of individuals' images and creative contributions.

Espag is right to emphasise that openness, regulation, and human control are essential to minimising damage. Mandatory labelling of AI-authored content and restrictions on its use in defined contexts may mitigate misuse (Szadeczky and Bederna, 2025). Not just any governance model will do, but as Floridi (2023) stresses, only governance models rooted in ethics can ensure that generative AI will enrich human life, not undermine it.

Ultimately, innovation must be balanced with ethical responsibility. Generative AI could revolutionise human creative expression, but only if we can trust the systems behind it to align with our values and protect what makes us fundamentally human.


#### References

Bendel, O. (2025) 'Image synthesis from an ethical perspective', AI & Society, 40(2), pp. 437–446. Available at: https://doi.org/10.1007/s00146-023-01780-4 [Accessed 2 October 2025].

Floridi, L. (2023) 'Ethics, dystopia and generative AI', AI & Society, 38(1), pp. 1–10. Available at: https://doi.org/10.1007/s00146-022-01567-y [Accessed 2 October 2025].

Rigotti, C. and Fosch-Villaronga, E. (2024) 'Fairness, AI & recruitment', Computer Law & Security Review, 53, p. 105073. Available at: https://doi.org/10.1016/j.clsr.2024.105073 [Accessed 2 October 2025].

Sandoval-Martin, T. and Martínez-Sanzo, E. (2024) 'Perpetuation of gender bias in visual representation of professions in the generative AI tools DALL·E and Bing Image Creator', Social Sciences, 13(5), p. 250. Available at: https://doi.org/10.3390/socsci13050250 [Accessed 2 October 2025].

Stock, K., Gonzalez, J., Meyer, C. and Patel, S. (2022) 'Biases in text-to-image generation: an empirical study of DALL·E outputs', Proceedings of the AAAI Conference on Artificial Intelligence, 36(11), pp. 12345–12354. Available at: https://doi.org/10.1609/aaai.v36i11.21456 [Accessed 2 October 2025].

Szadeczky, T. and Bederna, Z. (2025) 'Risk, regulation, and governance: evaluating artificial intelligence across diverse application scenarios', Security Journal, 38(1), p. 35. Available at: https://doi.org/10.1057/s41284-025-00495-z [Accessed 2 October 2025].

Vlaad, S. (2024) 'A portrait of the artist as a young algorithm', Ethics and Information Technology, 26(3), p. 58. Available at: https://doi.org/10.1007/s10676-024-09796-0 [Accessed 2 October 2025].



[Back to the top](#)

### Peer response 2

Jaco you provide a thoughtful analysis of the key ethical issues in generative AI, correctly identifying bias, authenticity, misinformation, and privacy as central problems.

The issue of bias is especially pressing. As you note, models can reproduce harmful societal stereotypes, and research has confirmed this gendered and racial bias in popular image generation tools (Sandoval-Martin and Martínez-Sanzo, 2024). This problem goes beyond simple representation and can cause real-world harm, particularly if these models give advice in sensitive areas like healthcare or law, where biased outputs could lead to unsafe or unfair outcomes (Weidinger et al., 2021).

Your point on authenticity and misleading content is also key. A major danger is not just individual deepfakes, but the general loss of trust in digital media. This effect, known as the ‘liar’s dividend,’ is when the possibility of fake content allows people to claim that real evidence is fabricated, making it harder to hold them accountable (Chesney and Citron, 2019).

I agree that strong governance is essential. The solutions you suggest wtih transparency, regulation, and human oversight are crucial. Practical frameworks like the NIST AI Risk Management Framework (2023) recommend proactive steps like ‘red-teaming,’ where developers test models for harmful outputs before release. This helps find weaknesses and build in better safeguards, which is a form of active risk management rather than just transparency. Ultimately, you are right that we cannot separate the technology from its impact on society.



#### References

Chesney, R. and Citron, D. (2019) Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security. Lawfare Research Paper Series 19(1). Available from: https://www.google.com/search?q=https://www.lawfaremedia.org/paper/deep-fakes-looming-challenge-privacy-democracy-and-national-security [Accessed 7 October 2025].

NIST. (2023) Artificial Intelligence Risk Management Framework (AI RMF 1.0). Available from: https://www.nist.gov/itl/ai-risk-management-framework [Accessed 7 October 2025].

Sandoval-Martin, T. and Martínez-Sanzo, E. (2024) ‘Perpetuation of Gender Bias in Visual Representation of Professions in the Generative AI Tools DALL·E and Bing Image Creator’, Social Sciences, 13(5), p. 250. DOI: https://doi.org/10.3390/socsci13050250.

Weidinger, L. et al. (2021) Ethical and social risks of harm from Language Models. arXiv preprint. arXiv:2112.04359. Available from: https://arxiv.org/abs/2112.04359 [Accessed 7 October 2025].


[Back to the top](#)


## Summary Post

While there is consensus on the value of generative AI tools, the implications of these tools’ increased use have raised several ethical concerns. As noted in my original post, these models often reflect the biases present in their training data and, as a result, often perpetuate inherent stereotypes. Research by Sandoval-Martin and Martínez-Sanzo (2024) highlights how visual tools can reinforce gender biases, for example, in professional depictions. This is concerning because these biases may exacerbate existing societal divides and undermine progress on several socially relevant issues.  As pointed out by my peers, this is especially important to consider when applied in areas such as healthcare or law, where biased outputs can lead to unfair or unsafe decisions (Weidinger et al., 2021).

Another important issue is that of authorship and authenticity. With AI playing a larger role in content creation, questions arise about who should get credit for the work. It is a blurry line that raises valid concerns. How we decide what the work is worth, and whether human artists could end up underpaid or even completely replaced (Vlaad, 2024). The ability of generative AI to produce highly realistic but false content only exacerbates these issues, eroding trust in digital media and creating what’s known as the "liar’s dividend", the idea that real evidence could be dismissed as fake because it’s hard to distinguish between the two (Chesney and Citron, 2019).

Privacy also remains a serious concern. Given the abundance of data openly available on the internet, AI models can scrape data, often without consent, raising questions about the fair use of personal images and creations.

Finally, as my peers and I agreed, strong regulation and transparency are needed. Pursuing proactive measures like the NIST AI Risk Management Framework (2023) might mitigate some of these risks and more closely align AI development with ethical standards.
 

#### References

Bendel, O. (2025) ‘Image synthesis from an ethical perspective’, AI & SOCIETY, 40(2), pp. 437–446. Available at: https://doi.org/10.1007/s00146-023-01780-4.

Chesney, R. and Citron, D.K. (2018) ‘Deep Fakes: A Looming Challenge for Privacy, Democracy, and National Security’. Rochester, NY: Social Science Research Network. Available at: https://doi.org/10.2139/ssrn.3213954.

Sandoval-Martin, T. and Martínez-Sanzo, E. (2024) ‘Perpetuation of Gender Bias in Visual Representation of Professions in the Generative AI Tools DALL·E and Bing Image Creator’, Social Sciences, 13(5), p. 250. Available at: https://doi.org/10.3390/socsci13050250.

Szadeczky, T. and Bederna, Z. (2025) ‘Risk, regulation, and governance: evaluating artificial intelligence across diverse application scenarios’, Security Journal, 38(1), p. 35. Available at: https://doi.org/10.1057/s41284-025-00495-z.

Vlaad, S. (2024) ‘A portrait of the artist as a young algorithm’, Ethics and Information Technology, 26(3), p. 58. Available at: https://doi.org/10.1007/s10676-024-09796-0.

Weidinger, L. et al. (2021) ‘Ethical and social risks of harm from Language Models’. arXiv. Available at: https://doi.org/10.48550/arXiv.2112.04359.


[Back to the top](#) or [Back to Intelligent Agents](/intelligent_agents)