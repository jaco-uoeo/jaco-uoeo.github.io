# Deep Learning in Action
{: .hidden-title }

## Activity
In this activity you are required to research an application of Deep Learning that you think is going to have an impact on society (whether that is positive or negative). When you have found your application you should make a post on the discussion forum that covers:

- An overview of the technology (e.g., what it does).
- A brief synopsis of how it works.
- The potential impacts this will have in terms of ethics, privacy, social good, or any other socio-technical aspects that you feel are relevant.

## Learning Outcomes
- An understanding of the motivations for, and appropriate use of, agent-based computing.
- An understanding of the main agent models in use today and their grounding in artificial intelligence research.


## Deep Learning Application: Deepfakes and Synthetic Media

### Overview of the technology

Deepfakes are a form of synthetic media created using deep learning, typically through Generative Adversarial Networks (GANs) to replicate real people’s faces, voices, and other charcteristics. These techniques can create highly realistic videos or audio where a person appears to say or do things they never actually did. While initially explored for entertainment and accessibility purposes, deepfakes have since raised major concerns due to their potential for deception, identity misuse, and misinformation.


### A brief synopsis of how it works

Deepfake generation relies on deep learning architectures such as GANs, autoencoders, and more recently, convolutional vision transformers (CVTs) and CNN hybrids for both generation and detection (Soudy et al., 2024). In a GAN, the generator creates fake content, while the discriminator learns to distinguish between real and fake data, refining the model through adversarial training. Sophisticated detection frameworks now use multi-layered approaches to identify subtle artefacts in synthetic content, including facial inconsistencies and temporal irregularities (Rathoure et al., 2024; Raza et al., 2022).


### The potential impact of deepfakes
From an ethical perspective, deepfakes present a unique threat by enabling non-consensual identity manipulation, such as fake pornography or impersonation, often targeting women and public figures. As de Ruiter (2021) argues, the harm of deepfakes goes beyond simple deception, it involves a violation of autonomy and dignity, even if the content is never publicly shared.

Deepfakes also pose democratic and societal risks, including political misinformation, reputational harm, and reduced trust in digital media. However, research efforts continue to advance detection methods to combat these threats (Raza et al., 2022; Soudy et al., 2024). A effective detection pipeline, such as that proposed by Rathoure et al. (2024), can support journalists, fact-checkers, and platforms in mitigating misuse.


### Conclusion

Deepfakes also pose democratic and societal risks, including political misinformation, reputational harm, and reduced trust in digital media. However, research efforts continue to advance detection methods to combat these threats (Raza et al., 2022; Soudy et al., 2024). Detection pipelins, such as that proposed by Rathoure et al. (2024), can support journalists, fact-checkers, and platforms in mitigating misuse.


### References

de Ruiter, A. (2021) ‘The Distinct Wrong of Deepfakes’, Philosophy & Technology, 34(4), pp. 1311–1332. Available at: https://doi.org/10.1007/s13347-021-00459-2.

Raza, A., Munir, K. and Almutairi, M. (2022) A Novel Deep Learning Approach for Deepfake Image Detection. Available at: https://www.mdpi.com/2076-3417/12/19/9820 (Accessed: 16 October 2025).

Rathoure, N. et al. (2024) ‘Combating deepfakes: a comprehensive multilayer deepfake video detection framework’, Multimedia Tools and Applications, 83(38), pp. 85619–85636. Available at: https://doi.org/10.1007/s11042-024-20012-5.

Soudy, A.H. et al. (2024) ‘Deepfake detection using convolutional vision transformers and convolutional neural networks’, Neural Computing and Applications, 36(31), pp. 19759–19775. Available at: https://doi.org/10.1007/s00521-024-10181-7.
