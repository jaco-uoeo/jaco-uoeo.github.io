# Convolutional Neural Networks
{: .hidden-title }

### Using CNNs for Law Enforcement: Ethical considerations

Facial recognition technology is being tested by police and security forces as a tool to spot criminals or potential terrorists. In reflecting on the article by Wall (2019), it is clear that while the idea of preventing attacks and saving lives is powerful, it comes with serious ethical risks. What if the system gets it wrong? A false match could mean an innocent person is wrongly targeted — or worse. These systems often struggle with accuracy, especially when identifying women and people with darker skin tones, because the data used to train them is often biased.

Real-world trials have shown high error rates, raising concerns about civil liberties and the potential for wrongful accusations. Some cities, like San Francisco, have already banned the use of facial recognition by authorities. Others are pressing ahead despite the risks, prompting warnings from AI experts about the consequences — not just for individuals, but for society as a whole.

As AI becomes more powerful, many believe it needs tighter regulation. Without oversight, there's a risk it could be misused or cause harm, especially in high-stakes areas like law enforcement or warfare. Experts argue that AI development should involve a range of voices — not just tech companies — to ensure fairness, accuracy, and accountability.

References

Wall, M. (2019) 'Biased and Wrong? Facial Recognition Tech in the Dock' Available at: https://www.bbc.com/news/business-48842750 (Accessed: 10 April 2025)

[Back to Machine Learning](/machine_learning/)