# Collaborative Discussion 2: Legal and Ethical views on ANN Applications 
{: .hidden-title }

## Brief
Read/listen to the article by Hutson (2021) in Nature on Robo-writers. This week, post your thoughts on the risks and benefits of the use AI writers at different levels, from administrative work to creative writing.

## Navigation

- [Initial Post](#initial-post)
  - [Peer Response 1](#peer-response-1)
- [Summary Post](#summary-post)
- [Back to Machine Learning](/machine_learning)


## Initial Post
Since Hudson's (2021) article, large language models (LLMs) have made significant strides in terms of complexity and performance. However, many of the challenges outlined then remain prevalent today. As highlighted by Mahowald et al. (2024), despite LLMs' impressive formal linguistic abilities, their ability to understand the content they generate remains limited. This gap often necessitates specialised fine-tuning to enhance their contextual comprehension and overall utility. Even with these measures, however, the occurrence of nonsensical or incoherent answers remains a persistent issue, underscoring the limitations of current LLM capabilities.

While nonsensical answers may often be easy to identify, content that appears linguistically valid can still be false in other ways and requires external verification (Huo, Arabzadeh and Clarke, 2023). Recent incidents highlighting the failure to verify generated output have raised concerns about the reliability of LLMs in legal practices and the potential consequences for law firms. A prominent example is Parker v Forsyth NO and Others in South Africa, where attorneys used ChatGPT to generate legal case citations (Phiri and Ramashia, 2023). The AI, however, produced fictitious case references, leading the court to impose costs on the plaintiff for presenting non-existent legal arguments. A similar issue arose in a lawsuit against Walmart, where LLM-generated case submissions included fabricated citations (Merken, 2025).

These incidents underscore the risks of relying on LLMs in legal contexts, as inaccuracies—such as fabricated citations or flawed legal arguments—can erode trust and lead to significant professional and financial consequences.


References

Huo, S., Arabzadeh, N. and Clarke, C.L.A. (2023) ‘Retrieving Supporting Evidence for LLMs Generated Answers’. arXiv. Available at: https://doi.org/10.48550/arXiv.2306.13781.

Hutson, M. (2021) ‘Robo-writers: the rise and risks of language-generating AI’, Nature, 591(7848), pp. 22–25. Available at: https://doi.org/10.1038/d41586-021-00530-0.

Mahowald, K. et al. (2024) ‘Dissociating language and thought in large language models’. arXiv. Available at: https://doi.org/10.48550/arXiv.2301.06627.

Merken, S. (2025) Lawyers in Walmart lawsuit admit AI ‘hallucinated’ case citations | Reuters. Available at: https://www.reuters.com/legal/legalindustry/lawyers-walmart-lawsuit-admit-ai-hallucinated-case-citations-2025-02-10/ (Accessed: 6 April 2025).

Phiri, N. and Ramashia, M. (2023) The use of ChatGPT in legal practice. Available at: https://www.polity.org.za/article/the-use-of-chatgpt-in-legal-practice-2023-10-04 (Accessed: 6 April 2025).

[Back to the top](#)

## Peer Responses

### Peer response 1
Thank you for your inputs. I think that it is crucial to discuss the reliability and accuracy of Large Language Models, especially as they are getting more traction every day being used across various tasks and domains. Despite their popularity LLMs remain limited by their inability to truly understand the content they generate.

Talukdar and Biswas (2024) propose a framework that uses what they call context awareness, which could serve as a first step towards improving both the reliability and the ethical alignment of LLMs. In addition to this framework, it is equally important to review the quality of the training data itself. Ensuring clean and high-quality training data is essential for training and building robust LLMs and it can help mitigate the risks associated with the models' missing understanding of the content it creates (Liu et al., 2024).

The issue you pointed out must also be adressed from a governance perspective. Clear legal frameworks and policies could play a key role in ensuring that content produced by LLMs has a certain standard of quality and accuracy. For instance, Huang et al. (2025) suggest that publicly available LLMs should undergo consistent validation by professionals.

I am very interested to see how these challenges will be addressed in the coming months and years.


References

Talukdar, W. and Biswas, A., 2024. Improving large language model (llm) fidelity through context-aware grounding: A systematic approach to reliability and veracity. arXiv preprint arXiv:2408.04023.

Liu, X., Liang, J., Ye, M. and Xi, Z., 2024. Robustifying safety-aligned large language models through clean data curation. arXiv preprint arXiv:2405.19358.

Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B. and Liu, T., 2025. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2), pp.1-55.

[Back to the top](#)


## Summary Post
Since Hudson’s 2021 article, large language models (LLMs) have come a long way in terms of complexity and capability. But despite these improvements, many of the core challenges remain. As Mahowald et al. (2024) point out, while LLMs can produce text that looks and sounds convincing, they still struggle with genuinely understanding what they generate. Because of this, developers often rely on fine-tuning to help these models better grasp context, though issues like incoherent or nonsensical responses still occur more often than one might expect.

More concerning is when models produce answers that seem plausible on the surface but are factually wrong. Huo, Arabzadeh, and Clarke (2023) stress the importance of external verification, especially when dealing with sensitive or high-stakes fields like law. There have already been real-world consequences. In the Parker v Forsyth NO and Others case in South Africa, attorneys used ChatGPT to generate legal citations—but some of those references turned out to be entirely made up (Phiri & Ramashia, 2023). A similar problem cropped up in a lawsuit against Walmart, where fabricated case law ended up in the submissions (Merken, 2025). In both situations, the errors led to professional and financial penalties.

A peer response to this discussion suggesting that frameworks like the one proposed by Talukdar and Biswas (2024), which focus on context awareness, could be a step in the right direction. They also emphasised the importance of using clean, high-quality training data (Liu et al., 2024) to reduce mistakes. Regulatory oversight could help—Huang et al. (2025) argue that experts should review public LLMs regularly to ensure accuracy and reliability.

In short, while LLMs are powerful tools, their limitations still demand technical improvements and thoughtful regulation.

References

Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B. and Liu, T., 2025. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2), pp.1-55.
Huo, S., Arabzadeh, N. and Clarke, C.L.A. (2023) ‘Retrieving Supporting Evidence for LLMs Generated Answers’. arXiv. Available at: https://doi.org/10.48550/arXiv.2306.13781.
Hutson, M. (2021) ‘Robo-writers: the rise and risks of language-generating AI’, Nature, 591(7848), pp. 22–25. Available at: https://doi.org/10.1038/d41586-021-00530-0.
Liu, X. et al. (2024) ‘Robustifying Safety-Aligned Large Language Models through Clean Data Curation’. arXiv. Available at: https://doi.org/10.48550/arXiv.2405.19358.
Mahowald, K. et al. (2024) ‘Dissociating language and thought in large language models’. arXiv. Available at: https://doi.org/10.48550/arXiv.2301.06627.
Merken, S. (2025) Lawyers in Walmart lawsuit admit AI ‘hallucinated’ case citations | Reuters. Available at: https://www.reuters.com/legal/legalindustry/lawyers-walmart-lawsuit-admit-ai-hallucinated-case-citations-2025-02-10/ (Accessed: 6 April 2025).
Phiri, N. and Ramashia, M. (2023) The use of ChatGPT in legal practice. Available at: https://www.polity.org.za/article/the-use-of-chatgpt-in-legal-practice-2023-10-04 (Accessed: 6 April 2025).
Talukdar, W. and Biswas, A. (2023) ‘Improving Large Language Model (LLM) fidelity through context-aware grounding: A systematic approach to reliability and veracity’, World Journal of Advanced Engineering Technology and Sciences, 10(2), pp. 283–296. Available at: https://doi.org/10.30574/wjaets.2023.10.2.0317.


[Back to the top](#) or [Back to Machine Learning](/machine_learning/)